{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import math\n",
    "from filelock import FileLock\n",
    "\n",
    "# __import_tune_begin__\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter, JupyterNotebookReporter\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback, TuneReportCheckpointCallback\n",
    "# __import_tune_end__\n",
    "\n",
    "\n",
    "from LightningMNISTClassifier import LightningMNISTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "logger = logging.getLogger('App')\n",
    "logging.basicConfig(level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_mnist_tune_checkpoint(config,\n",
    "                                checkpoint_dir=None,\n",
    "                                num_epochs=10,\n",
    "                                num_gpus=0):\n",
    "    data_dir = os.path.expanduser(\"~/data\")\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=num_epochs,\n",
    "        # If fractional GPUs passed in, convert to int.\n",
    "        gpus=math.ceil(num_gpus),\n",
    "        logger=TensorBoardLogger(save_dir=tune.get_trial_dir(), name=\"\", version=\".\"),\n",
    "        progress_bar_refresh_rate=config[\"progress_bar_refresh_rate\"],\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                metrics={\n",
    "                    \"loss\": \"ptl/val_loss\",\n",
    "                    \"mean_accuracy\": \"ptl/val_accuracy\"\n",
    "                },\n",
    "                filename=\"checkpoint\",\n",
    "                on=\"validation_end\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        model = LightningMNISTClassifier.load_from_checkpoint(os.path.join(checkpoint_dir, \"checkpoint\"), config=config,\n",
    "                                                              data_dir=data_dir)\n",
    "        logger.info('Lightning loaded from checkpoint')\n",
    "    else:\n",
    "        model = LightningMNISTClassifier(config=config, data_dir=data_dir)\n",
    "        logger.info('Lightning initialized')\n",
    "\n",
    "    trainer.fit(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def tune_mnist_pbt(num_samples=20, num_epochs=10, gpus_per_trial=0):\n",
    "    config = {\n",
    "        \"layer_1_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        \"layer_2_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        \"layer_1_size\": 512,\n",
    "        \"layer_2_size\": 512,\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 64,\n",
    "    }\n",
    "\n",
    "    def explore(config):\n",
    "        logger.info(\"======================================= EXPLORE =========================================\")\n",
    "        logger.info(config)\n",
    "        config['batch_size'] = config['batch_size'] + 10\n",
    "        return config\n",
    "\n",
    "    def generate_batch_sizes():\n",
    "        res = []\n",
    "        for _ in range(random.randint(1, 10)):\n",
    "            res.append(random.randint(8, 129))\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \"\"\"\n",
    "    hyperparam_mutations={\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"batch_size\": [32, 64, 128]\n",
    "    }\n",
    "    \"\"\"\n",
    "    scheduler = PopulationBasedTraining(\n",
    "        time_attr=\"training_iteration\",\n",
    "        perturbation_interval=1,\n",
    "        # Models will be considered for perturbation at this interval of time_attr=\"time_total_s\"\n",
    "        hyperparam_mutations={\n",
    "            \"batch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        },\n",
    "        custom_explore_fn=explore,\n",
    "        log_config=True\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    reporter_cli = CLIReporter(\n",
    "        parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
    "        metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    reporter_jupyter = JupyterNotebookReporter(\n",
    "      overwrite = False,\n",
    "      parameter_columns=[\"layer_1_size\", \"layer_2_size\", \"lr\", \"batch_size\"],\n",
    "      metric_columns=[\"loss\", \"mean_accuracy\", \"training_iteration\"]\n",
    "    )\n",
    "\n",
    "    analysis = tune.run(\n",
    "        tune.with_parameters(\n",
    "            train_mnist_tune_checkpoint,\n",
    "            num_epochs=num_epochs,\n",
    "            num_gpus=gpus_per_trial),\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 1,\n",
    "            \"gpu\": gpus_per_trial\n",
    "        },\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config={\n",
    "            \"progress_bar_refresh_rate\": 0,\n",
    "            \"layer_1_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "            \"layer_2_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "            \"lr\": tune.choice([1e-2, 1e-3, 1e-4, 1e-5, 1e-6]),\n",
    "            \"batch_size\": tune.choice([32, 64, 128, 256, 512, 1024]),\n",
    "        },\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        progress_reporter=reporter_jupyter,\n",
    "        verbose=1,\n",
    "        name=\"tune_mnist_pbt\",\n",
    "        stop={  # Stop a single trial if one of the conditions are met\n",
    "            \"mean_accuracy\": 0.98,\n",
    "            \"training_iteration\": 15},\n",
    "        local_dir=\"./data\",\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "    return analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 4.2/62.7 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 PENDING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m   return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 1 | layer_2 | Linear | 4.2 K \n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2 | layer_3 | Linear | 650   \n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 55.1 K    Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 55.1 K    Total params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0.220     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m 0.508     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 1 | layer_2 | Linear | 16.6 K\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 2 | layer_3 | Linear | 2.6 K \n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 69.5 K    Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 69.5 K    Total params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0.278     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 0 | layer_1 | Linear | 401 K \n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 1 | layer_2 | Linear | 65.7 K\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 2 | layer_3 | Linear | 1.3 K \n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 468 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 468 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 1.875     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0 | layer_1 | Linear | 803 K \n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 1 | layer_2 | Linear | 1.0 M \n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 1.9 M     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 1.9 M     Total params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 7.455     Total estimated model params size (MB)\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   rank_zero_warn(\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.3/62.7 GiB<br>PopulationBasedTraining: 0 checkpoints, 0 perturbs<br>Resources requested: 5.0/8 CPUs, 1.0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00001 with loss=2.1718108654022217 and parameters={'layer_1_size': 64, 'layer_2_size': 64, 'lr': 1e-05, 'batch_size': 256}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 RUNNING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   rank_zero_deprecation(\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001B[2m\u001B[36m(pid=63334)\u001B[0m   rank_zero_deprecation(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.2/62.7 GiB<br>PopulationBasedTraining: 3 checkpoints, 0 perturbs<br>Resources requested: 5.0/8 CPUs, 1.0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00000 with loss=0.1713031381368637 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 256}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 RUNNING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 18:59:03,689\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00000 (score -0.1713031381368637) -> train_mnist_tune_checkpoint_9e4dd_00001 (score -1.9344497919082642)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 128}\n",
      "2021-09-26 18:59:03,694\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 256} -> {'batch_size': 138}\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2021-09-26 18:59:03,706\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00001_1_batch_size=256,layer_1_size=64,layer_2_size=64,lr=1e-05_2021-09-26_18-58-41/checkpoint_tmpa3b2ab/./\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2021-09-26 18:59:03,706\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 1, '_timesteps_total': None, '_time_total': 13.041486740112305, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.3/62.7 GiB<br>PopulationBasedTraining: 5 checkpoints, 1 perturbs<br>Resources requested: 5.0/8 CPUs, 1.0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00000 with loss=0.1217290535569191 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 256}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 RUNNING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m /home/akaver/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/deprecated_api.py:25: LightningDeprecationWarning: `Trainer.running_sanity_check` has been renamed to `Trainer.sanity_checking` and will be removed in v1.5.\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   rank_zero_deprecation(\n",
      "2021-09-26 18:59:14,040\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00000 (score -0.10354693233966827) -> train_mnist_tune_checkpoint_9e4dd_00003 (score -1.6637017726898193)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 204}\n",
      "2021-09-26 18:59:14,046\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 256} -> {'batch_size': 214}\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2021-09-26 18:59:14,069\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00003_3_batch_size=32,layer_1_size=1024,layer_2_size=1024,lr=1e-06_2021-09-26_18-58-41/checkpoint_tmpd3dda0/./\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2021-09-26 18:59:14,069\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 29.94226360321045, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0.508     Total estimated model params size (MB)\n",
      "2021-09-26 18:59:19,887\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00000 (score -0.10354693233966827) -> train_mnist_tune_checkpoint_9e4dd_00004 (score -0.9840111136436462)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 204}\n",
      "2021-09-26 18:59:19,891\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 256} -> {'batch_size': 214}\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.3/62.7 GiB<br>PopulationBasedTraining: 6 checkpoints, 3 perturbs<br>Resources requested: 5.0/8 CPUs, 1.0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00000 with loss=0.10354693233966827 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 256}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 RUNNING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 2021-09-26 18:59:19,901\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00004_4_batch_size=512,layer_1_size=512,layer_2_size=128,lr=1e-05_2021-09-26_18-58-41/checkpoint_tmp2e8848/./\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 2021-09-26 18:59:19,901\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 29.94226360321045, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=64607)\u001B[0m 0.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 22.3/62.7 GiB<br>PopulationBasedTraining: 10 checkpoints, 3 perturbs<br>Resources requested: 5.0/8 CPUs, 1.0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00004 with loss=0.07100281864404678 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 214}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 RUNNING)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 18:59:31,134\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00004 (score -0.07100281864404678) -> train_mnist_tune_checkpoint_9e4dd_00002 (score -0.17781594395637512)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 256}\n",
      "2021-09-26 18:59:31,137\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 214} -> {'batch_size': 266}\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 2021-09-26 18:59:31,146\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00002_2_batch_size=256,layer_1_size=64,layer_2_size=256,lr=0.01_2021-09-26_18-58-41/checkpoint_tmp1d56ca/./\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 2021-09-26 18:59:31,146\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': None, '_time_total': 39.39965867996216, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=63248)\u001B[0m 0.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 18.8/62.7 GiB<br>PopulationBasedTraining: 11 checkpoints, 4 perturbs<br>Resources requested: 4.0/8 CPUs, 0.8/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00004 with loss=0.07463529706001282 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 214}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (4 RUNNING, 1 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 18:59:39,893\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00002 (score -0.0741184800863266) -> train_mnist_tune_checkpoint_9e4dd_00003 (score -0.1259935200214386)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 319}\n",
      "2021-09-26 18:59:39,895\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 266} -> {'batch_size': 329}\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2021-09-26 18:59:39,904\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00003_3_batch_size=32,layer_1_size=1024,layer_2_size=1024,lr=1e-06_2021-09-26_18-58-41/checkpoint_tmpbdf497/./\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2021-09-26 18:59:39,904\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 46.93543744087219, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=64606)\u001B[0m 0.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 18.8/62.7 GiB<br>PopulationBasedTraining: 12 checkpoints, 5 perturbs<br>Resources requested: 4.0/8 CPUs, 0.8/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00002 with loss=0.0741184800863266 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 266}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (4 RUNNING, 1 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 18:59:48,077\tINFO pbt.py:540 -- [exploit] transferring weights from trial train_mnist_tune_checkpoint_9e4dd_00002 (score -0.07930481433868408) -> train_mnist_tune_checkpoint_9e4dd_00001 (score -0.11694955825805664)\n",
      "INFO:App:======================================= EXPLORE =========================================\n",
      "INFO:App:{'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 319}\n",
      "2021-09-26 18:59:48,078\tINFO pbt.py:557 -- [explore] perturbed config from {'batch_size': 266} -> {'batch_size': 329}\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2021-09-26 18:59:48,084\tINFO trainable.py:382 -- Restored on 192.168.1.23 from checkpoint: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt/train_mnist_tune_checkpoint_9e4dd_00001_1_batch_size=256,layer_1_size=64,layer_2_size=64,lr=1e-05_2021-09-26_18-58-41/checkpoint_tmpa6ce6d/./\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2021-09-26 18:59:48,084\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 6, '_timesteps_total': None, '_time_total': 54.66638445854187, '_episodes_total': None}\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m GPU available: True, used: True\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m TPU available: False, using: 0 TPU cores\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m IPU available: False, using: 0 IPUs\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m \n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m   | Name    | Type   | Params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0 | layer_1 | Linear | 50.2 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 1 | layer_2 | Linear | 66.6 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 2 | layer_3 | Linear | 10.2 K\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m -----------------------------------\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 127 K     Trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0         Non-trainable params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 127 K     Total params\n",
      "\u001B[2m\u001B[36m(pid=63246)\u001B[0m 0.508     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 15.2/62.7 GiB<br>PopulationBasedTraining: 14 checkpoints, 6 perturbs<br>Resources requested: 3.0/8 CPUs, 0.6000000000000001/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00003 with loss=0.051499806344509125 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (3 RUNNING, 2 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 18:59:56,641\tINFO pbt.py:489 -- [pbt]: no checkpoint for trial. Skip exploit for Trial train_mnist_tune_checkpoint_9e4dd_00004\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 11.5/62.7 GiB<br>PopulationBasedTraining: 14 checkpoints, 6 perturbs<br>Resources requested: 2.0/8 CPUs, 0.4/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00001 with loss=0.04850537329912186 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (2 RUNNING, 3 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 7.8/62.7 GiB<br>PopulationBasedTraining: 14 checkpoints, 6 perturbs<br>Resources requested: 1.0/8 CPUs, 0.2/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00001 with loss=0.04850537329912186 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "== Status ==<br>Memory usage on this node: 7.7/62.7 GiB<br>PopulationBasedTraining: 14 checkpoints, 6 perturbs<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/35.77 GiB heap, 0.0/17.88 GiB objects (0.0/1.0 accelerator_type:G)<br>Current best trial: 9e4dd_00001 with loss=0.04850537329912186 and parameters={'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}<br>Result logdir: /home/akaver/!Dev/pbt-demo-mnist/data/tune_mnist_pbt<br>Number of trials: 5/5 (5 TERMINATED)<br><br>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-26 19:00:01,777\tINFO tune.py:550 -- Total run time: 80.89 seconds (80.76 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}\n",
      "{'9e4dd_00000': {'loss': 0.0987594947218895, 'mean_accuracy': 0.9701861143112183, 'time_this_iter_s': 9.433017492294312, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 5, 'experiment_id': 'eb2908eb5f734574af91336783829850', 'date': '2021-09-26_18-59-31', 'timestamp': 1632671971, 'time_total_s': 48.29545521736145, 'pid': 63334, 'hostname': 'ml-linux', 'node_ip': '192.168.1.23', 'config': {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 256}, 'time_since_restore': 48.29545521736145, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'trial_id': '9e4dd_00000', 'experiment_tag': '0_batch_size=256,layer_1_size=64,layer_2_size=1024,lr=0.001'}, '9e4dd_00001': {'loss': 0.04850537329912186, 'mean_accuracy': 0.982487678527832, 'time_this_iter_s': 5.989210844039917, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 7, 'experiment_id': 'eb2908eb5f734574af91336783829850', 'date': '2021-09-26_18-59-54', 'timestamp': 1632671994, 'time_total_s': 60.65559530258179, 'pid': 63246, 'hostname': 'ml-linux', 'node_ip': '192.168.1.23', 'config': {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}, 'time_since_restore': 5.989210844039917, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'trial_id': '9e4dd_00001', 'experiment_tag': '1_batch_size=256,layer_1_size=64,layer_2_size=64,lr=1e-05@perturbed[batch_size=329]'}, '9e4dd_00002': {'loss': 0.06684628874063492, 'mean_accuracy': 0.98214191198349, 'time_this_iter_s': 4.156997442245483, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 9, 'experiment_id': 'eb2908eb5f734574af91336783829850', 'date': '2021-09-26_19-00-01', 'timestamp': 1632672001, 'time_total_s': 69.90973162651062, 'pid': 63248, 'hostname': 'ml-linux', 'node_ip': '192.168.1.23', 'config': {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 266}, 'time_since_restore': 30.510072946548462, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'trial_id': '9e4dd_00002', 'experiment_tag': '2_batch_size=256,layer_1_size=64,layer_2_size=256,lr=0.01@perturbed[batch_size=266]'}, '9e4dd_00003': {'loss': 0.051499806344509125, 'mean_accuracy': 0.9838525652885437, 'time_this_iter_s': 7.305898189544678, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 6, 'experiment_id': 'eb2908eb5f734574af91336783829850', 'date': '2021-09-26_18-59-47', 'timestamp': 1632671987, 'time_total_s': 54.24133563041687, 'pid': 64606, 'hostname': 'ml-linux', 'node_ip': '192.168.1.23', 'config': {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 329}, 'time_since_restore': 7.305898189544678, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'trial_id': '9e4dd_00003', 'experiment_tag': '3_batch_size=32,layer_1_size=1024,layer_2_size=1024,lr=1e-06@perturbed[batch_size=329]'}, '9e4dd_00004': {'loss': 0.1001167744398117, 'mean_accuracy': 0.9704599380493164, 'time_this_iter_s': 5.326607942581177, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 8, 'experiment_id': 'eb2908eb5f734574af91336783829850', 'date': '2021-09-26_18-59-56', 'timestamp': 1632671996, 'time_total_s': 66.66958332061768, 'pid': 64607, 'hostname': 'ml-linux', 'node_ip': '192.168.1.23', 'config': {'progress_bar_refresh_rate': 0, 'layer_1_size': 64, 'layer_2_size': 1024, 'lr': 0.001, 'batch_size': 214}, 'time_since_restore': 36.72731971740723, 'timesteps_since_restore': 0, 'iterations_since_restore': 5, 'trial_id': '9e4dd_00004', 'experiment_tag': '4_batch_size=512,layer_1_size=512,layer_2_size=128,lr=1e-05@perturbed[batch_size=214]'}}\n"
     ]
    }
   ],
   "source": [
    "analysis = tune_mnist_pbt(num_samples=5, num_epochs=5, gpus_per_trial=0.2)\n",
    "\n",
    "print(analysis.results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}